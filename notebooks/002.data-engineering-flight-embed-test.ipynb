{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c88c0112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "root_dir = os.path.abspath(\"..\")\n",
    "sys.path.append(root_dir)\n",
    "dotenv_path = os.path.join(root_dir, \".env\")\n",
    "load_dotenv(dotenv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471b9551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127abbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARQUET_PATH = os.path.join(root_dir, \"data\", \"processed_flight_features_test.parquet\")\n",
    "OUTPUT_DIR = os.path.join(root_dir, \"data\", \"embedded_flight_feature_lite_test\")\n",
    "\n",
    "PARQUET_OUT_DIR = os.path.join(root_dir, \"data\", \"embedded_flight_feature_lite_parquet_test\")\n",
    "\n",
    "COL_NAME = \"flight_text\"\n",
    "MODEL_NAME = \"all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc189177",
   "metadata": {},
   "outputs": [],
   "source": [
    "scan = pl.scan_parquet(PARQUET_PATH).select([COL_NAME]).with_row_index(\"row_id\")\n",
    "row_count = scan.select(pl.len()).collect(engine=\"streaming\")[0, 0]\n",
    "print(f\"[INFO] Total rows: {row_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70515700",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610fc6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "chunk_idx = 0\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "while start < row_count:\n",
    "    end = min(start + BATCH_SIZE, row_count)\n",
    "    print(f\"[INFO] Processing rows {start} to {end} of {row_count}\")\n",
    "\n",
    "    # Collect this chunk\n",
    "    df_chunk = (\n",
    "        pl.scan_parquet(PARQUET_PATH)\n",
    "        .select([COL_NAME])\n",
    "        .with_row_index(\"row_id\")\n",
    "        .filter((pl.col(\"row_id\") >= start) & (pl.col(\"row_id\") < end))\n",
    "        .collect(engine=\"streaming\")\n",
    "    )\n",
    "\n",
    "    texts = df_chunk[COL_NAME].to_list()\n",
    "    row_ids = df_chunk[\"row_id\"].to_list()\n",
    "\n",
    "    # Encode in smaller batches\n",
    "    all_embeddings = []\n",
    "    for i in range(0, len(texts), BATCH_SIZE):\n",
    "        subtexts = texts[i:i+BATCH_SIZE]\n",
    "        emb = model.encode(\n",
    "            subtexts,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            show_progress_bar=False,\n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True\n",
    "        )\n",
    "        all_embeddings.append(emb)\n",
    "\n",
    "    all_embeddings = np.vstack(all_embeddings)  # shape: (chunk_size, 384)\n",
    "    # Save embeddings and row_ids\n",
    "    out_file = os.path.join(OUTPUT_DIR, f\"embeddings_part{chunk_idx:05d}.npz\")\n",
    "    np.savez_compressed(out_file, row_ids=np.array(row_ids), embeddings=all_embeddings)\n",
    "    print(f\"[INFO] Saved {len(row_ids)} embeddings to {out_file}\")\n",
    "\n",
    "    # Next chunk\n",
    "    start = end\n",
    "    chunk_idx += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e19503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all .npz files\n",
    "npz_files = sorted(OUTPUT_DIR.glob(\"embeddings_part*.npz\"))\n",
    "print(f\"[INFO] Found {len(npz_files)} chunk files\")\n",
    "\n",
    "all_tables = []\n",
    "\n",
    "for f in npz_files:\n",
    "    data = np.load(f)\n",
    "    row_ids = data[\"row_ids\"]           # shape (N,)\n",
    "    embeddings = data[\"embeddings\"]     # shape (N, 384)\n",
    "    n_samples, dim = embeddings.shape\n",
    "\n",
    "    # Build column names for embeddings\n",
    "    embed_cols = {f\"emb_{i}\": embeddings[:, i] for i in range(dim)}\n",
    "\n",
    "    # Create a Polars DataFrame for this chunk\n",
    "    df = pl.DataFrame({\n",
    "        \"row_id\": row_ids,\n",
    "        **embed_cols\n",
    "    })\n",
    "\n",
    "    all_tables.append(df)\n",
    "    print(f\"[INFO] Loaded {n_samples} embeddings from {f}\")\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "merged_df = pl.concat(all_tables, how=\"vertical\")\n",
    "\n",
    "# (Optional) sort by row_id if needed\n",
    "merged_df = merged_df.sort(\"row_id\")\n",
    "\n",
    "# Save to Parquet\n",
    "merged_df.write_parquet(MERGED_PARQUET)\n",
    "print(f\"[INFO] Saved merged embeddings to {MERGED_PARQUET}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
