{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c88c0112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "root_dir = os.path.abspath(\"..\")\n",
    "sys.path.append(root_dir)\n",
    "dotenv_path = os.path.join(root_dir, \".env\")\n",
    "load_dotenv(dotenv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471b9551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import math\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127abbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARQUET_PATH = os.path.join(root_dir, \"data\", \"processed_flight_features_test.parquet\")\n",
    "OUTPUT_DIR = os.path.join(root_dir, \"data\", \"embedded_flight_feature_lite_test\")\n",
    "\n",
    "PARQUET_OUT_DIR = os.path.join(root_dir, \"data\", \"embedded_flight_feature_lite_parquet_test\")\n",
    "\n",
    "COL_NAME = \"flight_text\"\n",
    "MODEL_NAME = \"all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc189177",
   "metadata": {},
   "outputs": [],
   "source": [
    "scan = pl.scan_parquet(PARQUET_PATH).select([COL_NAME]).with_row_index(\"row_id\")\n",
    "row_count = scan.select(pl.len()).collect(engine=\"streaming\")[0, 0]\n",
    "print(f\"[INFO] Total rows: {row_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70515700",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610fc6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "chunk_idx = 0\n",
    "BATCH_SIZE = 128\n",
    "ROW_COUNT = row_count\n",
    "\n",
    "READ_CHUNK_SIZE = 8192  # tune this based on memory\n",
    "\n",
    "while start < ROW_COUNT:\n",
    "    end = min(start + READ_CHUNK_SIZE, ROW_COUNT)\n",
    "    print(f\"[INFO] Loading rows {start} to {end} of {ROW_COUNT}\")\n",
    "\n",
    "    df_block = (\n",
    "        pl.scan_parquet(PARQUET_PATH)\n",
    "          .select([COL_NAME])\n",
    "          .with_row_index(\"row_id\")\n",
    "          .filter((pl.col(\"row_id\") >= start) & (pl.col(\"row_id\") < end))\n",
    "          .collect(engine=\"streaming\")\n",
    "    )\n",
    "\n",
    "    texts = df_block[COL_NAME].to_list()\n",
    "    row_ids = df_block[\"row_id\"].to_list()\n",
    "\n",
    "    for i in range(0, len(texts), BATCH_SIZE):\n",
    "        subtexts = texts[i:i+BATCH_SIZE]\n",
    "        subids = row_ids[i:i+BATCH_SIZE]\n",
    "        emb = model.encode(subtexts, batch_size=BATCH_SIZE,\n",
    "                           show_progress_bar=False,\n",
    "                           convert_to_numpy=True,\n",
    "                           normalize_embeddings=True)\n",
    "        out_file = os.path.join(OUTPUT_DIR, f\"embeddings_part{chunk_idx:05d}.npz\")\n",
    "        np.savez_compressed(out_file, row_ids=np.array(subids), embeddings=emb)\n",
    "        print(f\"[INFO] Saved {len(subtexts)} embeddings to {out_file}\")\n",
    "        chunk_idx += 1\n",
    "\n",
    "    start = end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e19503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all npz files\n",
    "NUM_OUTPUTS = 15\n",
    "\n",
    "npz_files = sorted(Path(OUTPUT_DIR).glob(\"embeddings_part*.npz\"))\n",
    "total_files = len(npz_files)\n",
    "print(f\"[INFO] Found {total_files} chunk files\")\n",
    "\n",
    "# how many files per parquet group (ceil)\n",
    "files_per_split = math.ceil(total_files / NUM_OUTPUTS)\n",
    "\n",
    "for split_idx in range(NUM_OUTPUTS):\n",
    "    start = split_idx * files_per_split\n",
    "    end = min((split_idx + 1) * files_per_split, total_files)\n",
    "    split_files = npz_files[start:end]\n",
    "\n",
    "    if not split_files:  # no files left\n",
    "        break\n",
    "\n",
    "    print(f\"[INFO] Processing split {split_idx+1}/{NUM_OUTPUTS}: files {start} to {end-1} ({len(split_files)} files)\")\n",
    "\n",
    "    all_tables = []\n",
    "\n",
    "    for f in split_files:\n",
    "        data = np.load(f)\n",
    "        row_ids = data[\"row_ids\"]\n",
    "        embeddings = data[\"embeddings\"]\n",
    "        n_samples, dim = embeddings.shape\n",
    "\n",
    "        embed_cols = {f\"emb_{i}\": embeddings[:, i] for i in range(dim)}\n",
    "\n",
    "        df = pl.DataFrame({\n",
    "            \"row_id\": row_ids,\n",
    "            **embed_cols\n",
    "        })\n",
    "        all_tables.append(df)\n",
    "\n",
    "    merged_df = pl.concat(all_tables, how=\"vertical\")\n",
    "    merged_df = merged_df.sort(\"row_id\")\n",
    "\n",
    "    out_path = os.path.join(PARQUET_OUT_DIR, f\"merged_part{split_idx:02d}.parquet\")\n",
    "    merged_df.write_parquet(out_path)\n",
    "    print(f\"[INFO] Saved split {split_idx+1} -> {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15761552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all parquet files (adjust pattern if needed)\n",
    "parquet_files = sorted(Path(PARQUET_OUT_DIR).glob(\"merged_part*.parquet\"))\n",
    "print(f\"[INFO] Found {len(parquet_files)} parquet files\")\n",
    "\n",
    "all_tables = []\n",
    "\n",
    "for f in parquet_files:\n",
    "    print(f\"[INFO] Loading {f}\")\n",
    "    df = pl.read_parquet(f)\n",
    "    all_tables.append(df)\n",
    "\n",
    "# Merge them into one DataFrame\n",
    "merged_df = pl.concat(all_tables, how=\"vertical\")\n",
    "\n",
    "# (Optional) sort by row_id if needed\n",
    "merged_df = merged_df.sort(\"row_id\")\n",
    "\n",
    "print(f\"[INFO] Final merged shape: {merged_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e526860",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = os.path.join(root_dir, \"data\", \"embed_flight_feature_test.parquet\")\n",
    "merged_df.write_parquet(out_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
