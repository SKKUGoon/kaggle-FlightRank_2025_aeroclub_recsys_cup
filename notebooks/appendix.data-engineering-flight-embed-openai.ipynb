{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af3af83",
   "metadata": {},
   "source": [
    "#### Because of timing constraint, wasn't able to perform data embedding from openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c88c0112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "root_dir = os.path.abspath(\"..\")\n",
    "sys.path.append(root_dir)\n",
    "dotenv_path = os.path.join(root_dir, \".env\")\n",
    "load_dotenv(dotenv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d941363",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import math\n",
    "import polars as pl\n",
    "import openai\n",
    "import json\n",
    "import glob\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c537a766",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARQUET_PATH = os.path.join(root_dir, \"data\", \"processed_flight_features_train.parquet\")\n",
    "BATCH_INPUT_JSONL = os.path.join(root_dir, \"data\", \"batch_input.jsonl\")\n",
    "BATCH_DIR = os.path.join(root_dir, \"data\", \"batch_job\")\n",
    "OUTPUT_DIR = os.path.join(root_dir, \"data\", \"embedded_flight_feature.parquet\")\n",
    "\n",
    "COL_NAME = \"flight_text\"\n",
    "MODEL_NAME = \"text-embedding-3-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9006814",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f785da65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Total rows: 18145372\n"
     ]
    }
   ],
   "source": [
    "scan = pl.scan_parquet(PARQUET_PATH).select([COL_NAME]).with_row_index(\"row_id\")\n",
    "row_count = scan.select(pl.len()).collect(engine=\"streaming\")[0, 0]\n",
    "print(f\"[INFO] Total rows: {row_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284d3779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute chunk size\n",
    "SPLIT = 400\n",
    "\n",
    "chunk_size = math.ceil(row_count / SPLIT)\n",
    "print(f\"[INFO] Each file will have up to {chunk_size} rows\")\n",
    "\n",
    "for split_idx in range(SPLIT):\n",
    "    start = split_idx * chunk_size\n",
    "    if start >= row_count:\n",
    "        break\n",
    "    end = min(start + chunk_size, row_count)\n",
    "\n",
    "    print(f\"[INFO] Processing rows {start}..{end} into part {split_idx}\")\n",
    "\n",
    "    # collect this chunk\n",
    "    batch_df = scan.slice(start, chunk_size).collect(streaming=True)\n",
    "    texts = batch_df[COL_NAME].to_list()\n",
    "    row_ids = batch_df[\"row_id\"].to_list()\n",
    "\n",
    "    # write to JSONL\n",
    "    out_path = os.path.join(BATCH_DIR, f\"batch_part_{split_idx}.jsonl\")\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for rid, txt in zip(row_ids, texts):\n",
    "            line = {\n",
    "                \"custom_id\": f\"row_{rid}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/embeddings\",\n",
    "                \"body\": {\n",
    "                    \"model\": MODEL_NAME,\n",
    "                    \"input\": txt\n",
    "                }\n",
    "            }\n",
    "            f.write(json.dumps(line, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"[INFO] Saved {len(row_ids)} rows to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a87799e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload JSONL as batch file\n",
    "client = openai.OpenAI()\n",
    "\n",
    "# Directory where you stored your chunked JSONL files\n",
    "CHUNK_PATTERN = os.path.join(BATCH_DIR, \"batch_part_*.jsonl\")\n",
    "\n",
    "# Get all chunk files (sorted to keep track)\n",
    "chunk_files = sorted(glob.glob(CHUNK_PATTERN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4b2c817",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchEmbed:\n",
    "    def __init__(self, jsonl_files, output_path: str | Path, file_tag: str):\n",
    "        \"\"\"\n",
    "        jsonl_files: list of up to 10 paths to your JSONL input files\n",
    "        \"\"\"\n",
    "        if len(jsonl_files) > 10:\n",
    "            raise ValueError(\"You can only handle up to 10 files per batch wave.\")\n",
    "        self.jsonl_files = jsonl_files\n",
    "        \n",
    "        # Internal memory\n",
    "        self.file_ids = []   # will hold OpenAI file IDs after send()\n",
    "        self.batch_ids = []  # will hold OpenAI batch IDs after batch()\n",
    "        \n",
    "        # Client\n",
    "        self.client = openai.OpenAI()\n",
    "        \n",
    "        # Save\n",
    "        self.output_path = output_path\n",
    "        self.file_tag = file_tag\n",
    "\n",
    "    def send(self):\n",
    "        \"\"\"Upload all JSONL files and save file IDs.\"\"\"\n",
    "        for path in self.jsonl_files:\n",
    "            with open(path, \"rb\") as f:\n",
    "                uploaded = self.client.files.create(file=f, purpose=\"batch\")\n",
    "                self.file_ids.append(uploaded.id)\n",
    "                print(f\"[INFO] Uploaded {path} ‚Üí file_id: {uploaded.id}\")\n",
    "        print(\"[INFO] All file_ids:\", self.file_ids)\n",
    "\n",
    "    def manual_file_id_insert(self, *file_ids):\n",
    "        self.file_ids = file_ids\n",
    "\n",
    "    def batch(self):\n",
    "        \"\"\"Create batch jobs from uploaded file IDs.\"\"\"\n",
    "        if not self.file_ids:\n",
    "            raise RuntimeError(\"No file_ids found. Run send() first.\")\n",
    "        for idx, fid in enumerate(self.file_ids):\n",
    "            batch = self.client.batches.create(\n",
    "                input_file_id=fid,\n",
    "                endpoint=\"/v1/embeddings\",\n",
    "                completion_window=\"24h\",\n",
    "                metadata={\"description\": f\"flight embeddings job part {idx}\"}\n",
    "            )\n",
    "            self.batch_ids.append(batch.id)\n",
    "            print(f\"[INFO] Created batch job for file_id {fid} ‚Üí batch_id: {batch.id}\")\n",
    "        print(\"[INFO] All batch_ids:\", self.batch_ids)\n",
    "\n",
    "    def manual_batch_id_insert(self, *batch_ids):\n",
    "        self.batch_ids = batch_ids\n",
    "\n",
    "    def retrieve(self):\n",
    "        \"\"\"\n",
    "        Retrieve results for each completed batch and write to parquet.\n",
    "        Run only after you confirm via dashboard that all batch_ids are completed.\n",
    "        \"\"\"\n",
    "        os.makedirs(self.output_path, exist_ok=True)\n",
    "        if not self.batch_ids:\n",
    "            raise RuntimeError(\"No batch_ids found. Run batch() first.\")\n",
    "\n",
    "        all_rows, all_embs = [], []\n",
    "        for bid in self.batch_ids:\n",
    "            status = self.client.batches.retrieve(bid)\n",
    "            if status.status != \"completed\":\n",
    "                print(f\"[WARN] Batch {bid} not completed (status={status.status}), skipping.\")\n",
    "                continue\n",
    "            output_file_id = status.output_file_id\n",
    "            print(f\"[INFO] Retrieving output for batch_id {bid}\")\n",
    "\n",
    "            output_content = self.client.files.content(output_file_id)\n",
    "\n",
    "            # parse lines\n",
    "            for line in output_content.iter_lines():\n",
    "                if not line:\n",
    "                    continue\n",
    "                data = json.loads(line)\n",
    "                if data.get(\"error\"):\n",
    "                    continue\n",
    "                rid = int(data[\"custom_id\"].replace(\"row_\", \"\"))\n",
    "                emb = data[\"response\"][\"body\"][\"data\"][0][\"embedding\"]\n",
    "                all_rows.append(rid)\n",
    "                all_embs.append(emb)\n",
    "\n",
    "        if not all_embs:\n",
    "            print(\"[WARN] No embeddings found in any batch.\")\n",
    "            return\n",
    "\n",
    "        # build one merged dataframe\n",
    "        emb_cols = {f\"emb_{j}\": [v[j] for v in all_embs] for j in range(len(all_embs[0]))}\n",
    "        df = pl.DataFrame({\"row_id\": all_rows, **emb_cols}).sort(\"row_id\")\n",
    "\n",
    "        out_path = os.path.join(self.output_path, f\"embeddings_{self.file_tag}.parquet\")\n",
    "        df.write_parquet(out_path)\n",
    "        print(f\"[INFO] Saved merged parquet with {len(all_rows)} rows ‚Üí {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd603a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create object\n",
    "START, END = 10, 20\n",
    "FILE_TAG = hashlib.sha256(f\"{START}_{END}\".encode()).hexdigest()\n",
    "\n",
    "embedder = BatchEmbed(chunk_files[START:END], OUTPUT_DIR, FILE_TAG)\n",
    "\n",
    "# 1Ô∏è‚É£ Upload files\n",
    "embedder.send()  \n",
    "# üëâ Save/backup printed file_ids somewhere safe\n",
    "\n",
    "# 2Ô∏è‚É£ Start batch jobs\n",
    "embedder.batch()\n",
    "# üëâ Save/backup printed batch_ids somewhere safe\n",
    "\n",
    "# 3Ô∏è‚É£ Later, after confirming completion in OpenAI dashboard:\n",
    "embedder.retrieve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db897694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Retrieving output for batch_id batch_687deaf1221881909a54e7f29c9a4ea7\n",
      "[INFO] Retrieving output for batch_id batch_687deae736648190b90313d7c362920a\n",
      "[INFO] Retrieving output for batch_id batch_687deade9e908190b72ccea7b1d3e551\n",
      "[INFO] Retrieving output for batch_id batch_687dead3b3a88190a9af44ab66e65a8d\n",
      "[INFO] Retrieving output for batch_id batch_687deacaabe08190b338c5db0bf26e75\n",
      "[INFO] Retrieving output for batch_id batch_687deabfc11881909545df5bfa6d5a90\n",
      "[INFO] Retrieving output for batch_id batch_687deab3db74819080aaf944aa255d53\n",
      "[INFO] Retrieving output for batch_id batch_687deaab1aec8190abf09cb13acb5756\n",
      "[INFO] Retrieving output for batch_id batch_687deaa275e08190a29f67bf9c9cb4be\n",
      "[INFO] Retrieving output for batch_id batch_687dea91e1c481908202a9c930b91dd1\n"
     ]
    }
   ],
   "source": [
    "# First batch usage\n",
    "START, END = 0, 10\n",
    "FILE_TAG = hashlib.sha256(f\"{START}_{END}\".encode()).hexdigest()\n",
    "embedder = BatchEmbed([], OUTPUT_DIR, FILE_TAG)\n",
    "\n",
    "embedder.manual_batch_id_insert(\n",
    "    \"batch_687deaf1221881909a54e7f29c9a4ea7\",\n",
    "    \"batch_687deae736648190b90313d7c362920a\",\n",
    "    \"batch_687deade9e908190b72ccea7b1d3e551\",\n",
    "    \"batch_687dead3b3a88190a9af44ab66e65a8d\",\n",
    "    \"batch_687deacaabe08190b338c5db0bf26e75\",\n",
    "    \"batch_687deabfc11881909545df5bfa6d5a90\",\n",
    "    \"batch_687deab3db74819080aaf944aa255d53\",\n",
    "    \"batch_687deaab1aec8190abf09cb13acb5756\",\n",
    "    \"batch_687deaa275e08190a29f67bf9c9cb4be\",\n",
    "    \"batch_687dea91e1c481908202a9c930b91dd1\"\n",
    ")\n",
    "embedder.retrieve()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d47a186",
   "metadata": {},
   "source": [
    "### Engineering Job Record\n",
    "\n",
    "| Batch Start, End | Start DT | End DT |\n",
    "|------------------|----------|--------|\n",
    "|0, 10| 25/07/21 16:20 | 25/07/22 10:30 |\n",
    "|10, 20 | 25/07/22 10:30 |  |\n",
    "|20, 30 |              |       |\n",
    "|30, 40 |              |       |\n",
    "|40, 50 |              |       |\n",
    "|50, 60 |              |       |\n",
    "|60, 70 |              |       |\n",
    "|70, 80 |              |       |\n",
    "|80, 90 |              |       |\n",
    "|90, 100 |              |       |\n",
    "|100, 110 |              |       |\n",
    "|110, 120 |              |       |\n",
    "|120, 130 |              |       |\n",
    "|130, 140 |              |       |\n",
    "|140, 150 |              |       |\n",
    "|150, 160 |              |       |\n",
    "|160, 170 |              |       |\n",
    "|170, 180 |              |       |\n",
    "|180, 190 |              |       |\n",
    "|190, 200 |              |       |\n",
    "|200, 210 |              |       |\n",
    "|210, 220 |              |       |\n",
    "|220, 230 |              |       |\n",
    "|230, 240 |              |       |\n",
    "|240, 250 |              |       |\n",
    "|250, 260 |              |       |\n",
    "|260, 270 |              |       |\n",
    "|270, 280 |              |       |\n",
    "|280, 290 |              |       |\n",
    "|290, 300 |              |       |\n",
    "|300, 310 |              |       |\n",
    "|310, 320 |              |       |\n",
    "|320, 330 |              |       |\n",
    "|330, 340 |              |       |\n",
    "|340, 350 |              |       |\n",
    "|350, 360 |              |       |\n",
    "|360, 370 |              |       |\n",
    "|370, 380 |              |       |\n",
    "|380, 390 |              |       |\n",
    "|390, 400 |              |       |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552c58b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c16773fa",
   "metadata": {},
   "source": [
    "## Optional - Embedding with HuggingFace Qwen\n",
    "\n",
    "* Couldn't accomplish this with my Mac\n",
    "\n",
    "```python\n",
    "# Config\n",
    "PARQUET_PATH = os.path.join(root_dir, \"data\", \"processed_flight_features_train.parquet\")\n",
    "OUTPUT_DIR = os.path.join(root_dir, \"data\", \"embed\")\n",
    "BATCH_SIZE = 32\n",
    "CHUNK_SIZE = 8192\n",
    "DEVICE = \"mps\"\n",
    "COL_NAME = \"flight_text\"\n",
    "MODEL = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "\n",
    "# Load the model\n",
    "model = SentenceTransformer(MODEL)\n",
    "model.to(torch.device(DEVICE))  # move model to Apple GPU\n",
    "\n",
    "scan = pl.scan_parquet(PARQUET_PATH).select([COL_NAME])\n",
    "row_count = scan.select(pl.len()).collect(engine=\"streaming\")[0, 0]\n",
    "print(f\"[INFO] Total rows: {row_count}\")\n",
    "\n",
    "for start in range(0, row_count, CHUNK_SIZE):\n",
    "    print(f\"[INFO] Processing rows {start}..{min(start+CHUNK_SIZE, row_count)}\")\n",
    "\n",
    "    # load a chunk from parquet\n",
    "    batch_df = scan.slice(start, CHUNK_SIZE).collect(engine=\"streaming\")\n",
    "    texts = batch_df.get_column(COL_NAME).to_list()\n",
    "    del batch_df  # free as soon as possible\n",
    "\n",
    "    # ‚úÖ embed with no_grad to save memory\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.encode(\n",
    "            texts,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            device=DEVICE,\n",
    "            convert_to_tensor=True,\n",
    "            show_progress_bar=True,\n",
    "            normalize_embeddings=True,\n",
    "        )\n",
    "\n",
    "    # ‚úÖ save and release GPU memory immediately\n",
    "    np.save(os.path.join(OUTPUT_DIR, f\"embeddings_part_{start//CHUNK_SIZE}.npy\"),\n",
    "            embeddings.cpu().numpy())\n",
    "    print(f\"[INFO] Saved part {start//CHUNK_SIZE}\")\n",
    "\n",
    "    # ‚úÖ release references and clear MPS cache\n",
    "    del texts\n",
    "    del embeddings\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "print(\"[‚úÖ] All done.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3038ccda",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
